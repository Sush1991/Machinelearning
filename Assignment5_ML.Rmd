---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(sparcl)     #to create colourDendograms
```

```{r}
Csdata<- read.csv("Cereals.csv")
```

```{r}
Csdata1<- na.omit(Csdata) ##remove the missing value      
```

```{r}
Csdata2<-Csdata1[,c(-2,-3)]
head(Csdata2)
```

```{r}
str(Csdata2)
summary(Csdata2)
```

```{r}
rownames(Csdata2) <- Csdata2$name ##Convert the names of the breakfast cereals to the row names, as this will later help us in visualising the clusters 
```


```{r}
Csdata2 <- Csdata2[,c(-1)] ##Drop the name column as it is now just redundant information
```


```{r}

##The data must be scaled, before measuring any type of distance metric as the variables with higher ranges will significantly influence the distance
Csdata2 <- scale(Csdata2)
head(Csdata2) 
```


Qs1 Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from  single linkage, complete linkage, average linkage and Ward. Choose the best method.

```{r}
# Dissimilarity matrix
d <- dist(Csdata2, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc_complete <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc_complete, cex = 0.6, hang = -1)

```

Using Agnes to compare the clustering from  single linkage, complete linkage, average linkage, and Ward and comparing agglomerative coefficients of each method.

```{r}
hc_single <- agnes(Csdata2, method = "single")
pltree(hc_single, cex = 0.6, hang = -1, main = "Dendrogram of agnes")

```

```{r}
hc_average <- agnes(Csdata2, method = "average")
pltree(hc_average, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

```{r}
# Compute with agnes
hc_complete <- agnes(Csdata2, method = "complete")

# Agglomerative coefficient
hc_complete$ac

```


```{r}
##We will find the agnes coefficient of all the methods.

library(tidyverse)
# methods to assess

m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(Csdata2, method = x)$ac
}

map_dbl(m, ac)
```

The best linkage method is ward with agglomerative coefficient of 0.9046042.

```{r}
hc_ward <- agnes(Csdata2, method = "ward")
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes") ##visualizing the dendrogram using wards method:
```

Qs 2) How many clusters would you choose?

Referring the dendrogram, a suitable height shall be chosen where a horizontal line shall be plotted and the number of instances where the horizontal line cut the branches of dendrogram are clusters of our data. So, our hierarchical clustering analysis is producing 6 clusters.
```{r}

#Create the distance matrix
d <- dist(Csdata2, method = "euclidean")

# Ward's method for Hierarchical clustering
hc_ward_cut <- hclust(d, method = "ward.D2" )

plot(hc_ward_cut, cex=0.6 )
rect.hclust(hc_ward_cut,k=6,border = 1:6) 

```

```{r}
##Lets see how many number of records of the data grouped and assigned  to clusters
# Cut tree into 6 groups
sub_grp <- cutree(hc_ward_cut, k = 6)

# Number of members in each cluster
table(sub_grp)
```

```{r}
fviz_cluster(list(data = Csdata2, cluster = sub_grp))##to  visualize the result in a plot.
```
```{r}
##install.packages("GGally")
library(GGally)
library(dplyr)
Csdata %>% 
  select(calories, protein, fat, sodium, fiber, carbo, sugars, potass,vitamins,rating) %>% 
  ggcorr(palette = "RdBu", label = TRUE, label_round =  2)
```

The correlation matrix  helps to find strong or weak relation existing between the variables. This will give us a better perspective to find descriptive statistics between the variables.
The cluster stability of each cluster in the original clustering is the mean value of its
Jaccard coefficient over all the bootstrap iterations. 
1) Clusters with a stability value less than 0.6 should be considered unstable. 
2)  Values between 0.6 and 0.75 indicate that the cluster is measuring a pattern in the data, but there isn't high certainty
about which points should be clustered together. 
3) Clusters with stability values above about 0.85 can be considered highly stable


```{r}
library(fpc)
library(cluster)
kbest <-6
cboot.hclust <- clusterboot(Csdata2,clustermethod=hclustCBI,method="ward.D2", k=kbest)
summary(cboot.hclust$result)
groups<-cboot.hclust$result$partition
```


```{r}
head(data.frame(groups))
##The vector of cluster stabilities
cboot.hclust$bootmean 

```

1) As we can see here there is a strong correlation between dietary fiber and potassium.
2) There are groups of cereals from which we can choose according to our preferences.  
4) Healthy cereals with much dietary fibers, less calories and less fats : 100% Bran, All-Bran with extra fibers, and All-Bran.
5) Cereals for hungry people in need of energy are Muesli Cereals

```{r}
cboot.hclust$bootbrd ##The count of how many times each cluster was dissolved. By default clusterboot() runs 100 bootstrap iterations.
```


The clusters contain nutritionally rich, adequate and poor levels. We grouped all the records in the 6 clusters


